{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header\n",
    "\n",
    "preprocess to generate `/data/GC_output/analysis/percent2` dataset\n",
    "\n",
    "forked from `preprocess.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import his_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only avg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.5_9_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.5_1_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.3_9_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.01_9_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.1_5_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.3_1_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.4_9_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.01_7_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.1_1_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.5_3_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.1_9_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.4_5_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.01_5_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.5_5_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.2_3_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.3_7_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.2_1_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.1_3_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.4_7_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.2_9_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.01_3_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.2_7_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.1_7_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.3_5_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.4_3_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.3_3_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.01_1_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.4_1_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.2_5_0.ncprocessing /geodata2/S2S/DL/GC_input/shuffle/ERA5_0.5_7_0.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'time': length 10 on 'time' and length 1 on {'lat': 'lat', 'lon': 'lon', 'level': 'level', 'time': '10m_u_component_of_wind'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_4039312/828030371.py\", line 73, in process_file\n    dataset_raw = his_preprocess.preprocess_GC(xr.open_dataset(file, engine=\"netcdf4\"), target_var)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hiskim1/graphcast/lib/his_preprocess.py\", line 129, in preprocess_GC\n    dataset[\"time\"] = pd.date_range(\"2021-06-22\", periods=10, freq=\"1D\")\n    ~~~~~~~^^^^^^^^\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/dataset.py\", line 1604, in __setitem__\n    self.update({key: value})\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/dataset.py\", line 5624, in update\n    merge_result = dataset_update_method(self, other)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/merge.py\", line 1075, in dataset_update_method\n    return merge_core(\n           ^^^^^^^^^^^\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/merge.py\", line 724, in merge_core\n    dims = calculate_dimensions(variables)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/variable.py\", line 3008, in calculate_dimensions\n    raise ValueError(\nValueError: conflicting sizes for dimension 'time': length 10 on 'time' and length 1 on {'lat': 'lat', 'lon': 'lon', 'level': 'level', 'time': '10m_u_component_of_wind'}\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m: (label, color, his_preprocess\u001b[38;5;241m.\u001b[39mweighted_mean(dataset_raw))}\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 78\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(process_file, perturb_files)\n\u001b[1;32m     80\u001b[0m perturb_datasets_mean \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# perturb_datasets_raw = [result[\"raw\"] for result in results]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'time': length 10 on 'time' and length 1 on {'lat': 'lat', 'lon': 'lon', 'level': 'level', 'time': '10m_u_component_of_wind'}"
     ]
    }
   ],
   "source": [
    "# GC 데이터 전처리\n",
    "\n",
    "\n",
    "# {gaussian scale}_{# values}_{ens i}.nc\n",
    "\n",
    "scale_list = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "file_dic ={}\n",
    "for target_var in [\"2m_temperature\"]:\n",
    "\n",
    "    if target_var == '2m_temperature':\n",
    "        # file_dic[0] = sorted(glob.glob('/data/GC_output/percent2/GC_11111111111_250_*.nc'))\n",
    "        for i, scale in enumerate(scale_list):\n",
    "            file_dic[i] = sorted(glob.glob(f'/geodata2/S2S/DL/GC_input/shuffle/ERA5_{scale}_*_0.nc'))\n",
    "            #print(file_dic[i])\n",
    "\n",
    "    # Assign base colors for each partition\n",
    "    colors_list = [\n",
    "        '#008000', # 초록\n",
    "        '#FF00FF', # 마젠타\n",
    "        '#0000FF', # 파랑\n",
    "        '#800080', # 보라\n",
    "        '#FF69B4', # 핫핑크\n",
    "        '#FFD700', # 골드\n",
    "        '#FF0000', # 빨강\n",
    "        '#00FF00', # 라임\n",
    "        '#00FFFF', # 시안\n",
    "        '#FFA500', # 주황\n",
    "        '#800000', # 마룬\n",
    "        '#4B0082', # 인디고\n",
    "        '#8B4513', # 새들브라운\n",
    "        '#FF4500', # 오렌지레드\n",
    "        '#00FF7F', # 스프링그린\n",
    "        '#1E90FF', # 도저블루\n",
    "        '#FF1493', # 딥핑크\n",
    "        '#7B68EE', # 미디엄슬레이트블루\n",
    "        '#20B2AA', # 라이트시그린\n",
    "        '#DAA520' # 골든로드\n",
    "    ]\n",
    "\n",
    "    def extract_perturbation_info(filename):\n",
    "        match = filename.split(\"/\")[-1].split(\"_\")\n",
    "        if match:\n",
    "            scale = match[1]  # perturbation 코드\n",
    "            percent = match[2]  # 값 (예: 0.001)\n",
    "            ens = match[3]  # 지역 코드 (예: 9p)\n",
    "            #print(f\"{scale}_{percent}_{ens}\")\n",
    "            return f\"{scale}_{percent}_{ens}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Collect perturbation files with labels and colors\n",
    "    perturb_files = []\n",
    "    \n",
    "    for i, partition_files in file_dic.items():\n",
    "        base_color = colors_list[i % len(colors_list)]\n",
    "        num_files = len(partition_files)\n",
    "        # Generate different shades of the base color\n",
    "        colors = sns.light_palette(base_color, n_colors=num_files + 2)[1:-1]\n",
    "        for i, file in enumerate(partition_files):\n",
    "            perturb_info = extract_perturbation_info(file)\n",
    "            if perturb_info:\n",
    "                label = f\"{perturb_info}\"\n",
    "                color = colors[i % len(colors_list)]\n",
    "                perturb_files.append((label, color, file))\n",
    "\n",
    "    # perturb_datasets_raw = []\n",
    "    perturb_datasets_mean = []\n",
    "\n",
    "    def process_file(file_info):\n",
    "        label, color, file = file_info\n",
    "        print(f\"processing {file}\")\n",
    "        dataset_raw = his_preprocess.preprocess_GC(xr.open_dataset(file, engine=\"netcdf4\"), target_var)\n",
    "        # return {\"mean\": (label, color, his_preprocess.weighted_mean(dataset_raw)), \"raw\": (label, color, dataset_raw)}\n",
    "        return {\"mean\": (label, color, his_preprocess.weighted_mean(dataset_raw))}\n",
    "\n",
    "    with Pool() as executor:\n",
    "        results = executor.map(process_file, perturb_files)\n",
    "\n",
    "    perturb_datasets_mean = [result[\"mean\"] for result in results]\n",
    "    # perturb_datasets_raw = [result[\"raw\"] for result in results]\n",
    "\n",
    "    if target_var == '2m_temperature':\n",
    "        with open('/data/GC_output/analysis/shuffle/ERA5_t2m_GlobAvg.pkl', 'wb') as f:\n",
    "            pickle.dump(perturb_datasets_mean, f)\n",
    "        # with open('/data/GC_output/analysis/shuffle/sparse_GC_t2m_Globraw.pkl', 'wb') as f:\n",
    "        #     pickle.dump(perturb_datasets_raw, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Raw File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GC 데이터 전처리\n",
    "\n",
    "# {gaussian scale}_{# values}_{ens i}.nc\n",
    "\n",
    "scale_list = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "file_dic ={}\n",
    "for target_var in [\"2m_temperature\"]:\n",
    "\n",
    "    if target_var == '2m_temperature':\n",
    "        # file_dic[0] = sorted(glob.glob('/data/GC_output/percent2/GC_11111111111_250_*.nc'))\n",
    "        for i, scale in enumerate(scale_list):\n",
    "            file_dic[i] = sorted(glob.glob(f'/data/GC_output/shuffle/GC_{scale}_*_*.nc'))\n",
    "            # print(file_dic[i])\n",
    "\n",
    "    # Assign base colors for each partition\n",
    "    colors_list = [\n",
    "        '#008000', # 초록\n",
    "        '#FF00FF', # 마젠타\n",
    "        '#0000FF', # 파랑\n",
    "        '#800080', # 보라\n",
    "        '#FF69B4', # 핫핑크\n",
    "        '#FFD700', # 골드\n",
    "        '#FF0000', # 빨강\n",
    "        '#00FF00', # 라임\n",
    "        '#00FFFF', # 시안\n",
    "        '#FFA500', # 주황\n",
    "        '#800000', # 마룬\n",
    "        '#4B0082', # 인디고\n",
    "        '#8B4513', # 새들브라운\n",
    "        '#FF4500', # 오렌지레드\n",
    "        '#00FF7F', # 스프링그린\n",
    "        '#1E90FF', # 도저블루\n",
    "        '#FF1493', # 딥핑크\n",
    "        '#7B68EE', # 미디엄슬레이트블루\n",
    "        '#20B2AA', # 라이트시그린\n",
    "        '#DAA520' # 골든로드\n",
    "    ]\n",
    "\n",
    "    def extract_perturbation_info(filename):\n",
    "        match = filename.split(\"/\")[-1].split(\"_\")\n",
    "        if match:\n",
    "            scale = match[1]  # perturbation 코드\n",
    "            percent = match[2]  # 값 (예: 0.001)\n",
    "            ens = match[3]  # 지역 코드 (예: 9p)\n",
    "            # print(f\"{scale}_{percent}_{ens}\")\n",
    "            return f\"{scale}_{percent}_{ens}\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Collect perturbation files with labels and colors\n",
    "    perturb_files = []\n",
    "    \n",
    "    for i, partition_files in file_dic.items():\n",
    "        base_color = colors_list[i % len(colors_list)]\n",
    "        num_files = len(partition_files)\n",
    "        # Generate different shades of the base color\n",
    "        colors = sns.light_palette(base_color, n_colors=num_files + 2)[1:-1]\n",
    "        for i, file in enumerate(partition_files):\n",
    "            perturb_info = extract_perturbation_info(file)\n",
    "            if perturb_info:\n",
    "                label = f\"{perturb_info}\"\n",
    "                color = colors[i % len(colors_list)]\n",
    "                perturb_files.append((label, color, file))\n",
    "\n",
    "    perturb_datasets_raw = []\n",
    "    perturb_datasets_mean = []\n",
    "\n",
    "    def process_file(file_info):\n",
    "        label, color, file = file_info\n",
    "        dataset_raw = his_preprocess.preprocess_GC(xr.open_dataset(file, engine=\"netcdf4\"), target_var)\n",
    "        # return {\"mean\": (label, color, his_preprocess.weighted_mean(dataset_raw)), \"raw\": (label, color, dataset_raw)}\n",
    "        return {\"raw\": (label, color, dataset_raw)}\n",
    "\n",
    "    with Pool(processes=13) as executor:\n",
    "        results = executor.map(process_file, perturb_files)\n",
    "\n",
    "    # perturb_datasets_mean = [result[\"mean\"] for result in results]\n",
    "    perturb_datasets_raw = [result[\"raw\"] for result in results]\n",
    "\n",
    "    if target_var == '2m_temperature':\n",
    "        # with open('/data/GC_output/analysis/shuffle/full_GC_t2m_GlobAvg.pkl', 'wb') as f:\n",
    "        #     pickle.dump(perturb_datasets_mean, f)\n",
    "        with open('/data/GC_output/analysis/shuffle/full_GC_t2m_Globraw.pkl', 'wb') as f:\n",
    "            pickle.dump(perturb_datasets_raw, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiskim1_graphcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
