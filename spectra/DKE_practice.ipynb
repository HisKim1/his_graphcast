{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header\n",
    "\n",
    "DKE가 뭔지 여기서 차근차근 알아보도록 하자. \n",
    "paper가 준 data를 받아서 어떻게 만들었는지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The paper is reproducible!\n",
    "\n",
    "이렇게 하니까 Fig 1과 동일한 그림을 그릴 수 있어여ㅛ!\n",
    "```python\n",
    "dataset = xr.open_dataset('dataset_paper/pangu1000.nc')\n",
    "u = dataset['u'].var(dim=\"ens\")\n",
    "v = dataset['v'].var(dim=\"ens\")\n",
    "dke = u + v\n",
    "\n",
    "weights = np.cos(np.deg2rad(dke.lat))\n",
    "weights.name = \"weights\"\n",
    "dke_weighted = dke.weighted(weights)\n",
    "dke_weighted = dke_weighted.mean(dim=(\"lat\", \"lon\"))\n",
    "\n",
    "# plot in y-axis log scale\n",
    "dke_weighted.plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset('dataset_paper/pangu1000/pangu1000.nc')\n",
    "u = dataset['u'].var(dim=\"ens\")\n",
    "v = dataset['v'].var(dim=\"ens\")\n",
    "\n",
    "dke = u + v # (73, 721, 1440)\n",
    "\n",
    "# calculate the weighted mean\n",
    "weights = np.cos(np.deg2rad(dke.lat))\n",
    "weights.name = \"weights\"\n",
    "dke_weighted = dke.weighted(weights)\n",
    "dke_weighted = dke_weighted.mean(dim=(\"lat\", \"lon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dke in log-scale y axis\n",
    "\n",
    "# increase the figure font size\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "# plt.plot(np.arange(0, 73, 1), dke_weighted, color=\"red\")\n",
    "\n",
    "dataset = xr.open_zarr(\"/geodata2/Gencast/output/2021-06-21.zarr\").sel(level=300).squeeze()\n",
    "u = dataset['10m_u_component_of_wind'].var(dim=\"sample\")\n",
    "v = dataset['10m_v_component_of_wind'].var(dim=\"sample\")\n",
    "dke = u + v # (73, 721, 1440)\n",
    "\n",
    "# calculate the weighted mean\n",
    "weights = np.cos(np.deg2rad(dke.lat))\n",
    "weights.name = \"weights\"\n",
    "dke_weighted = dke.weighted(weights)\n",
    "dke_weighted = dke_weighted.mean(dim=(\"lat\", \"lon\"))\n",
    "\n",
    "plt.plot(np.arange(0, 12, 1)*12, dke_weighted.isel(time=slice(0, 12)), color=\"blue\", label=\"GenCast\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# x axis: 0 ~ 72hr, hourly\n",
    "plt.xticks(np.arange(0, 73, 12))\n",
    "plt.xlim(0, 73)\n",
    "plt.xlabel(\"forecast lead time (h)\")\n",
    "# plt.ylim(10**-6, 30)\n",
    "plt.ylabel(\"DKE (m^2/s^2)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"dke_weighted.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    " cdo remapbil,n360 pangu1000_ensavg.nc tmp.nc\n",
    " cdo uv2dv,linear tmp.nc pangu1000_dv.nc\n",
    " rm -f tmp.nc\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset3 = xr.open_dataset(\"/geodata2/S2S/DL/GC_input/2021-06-21/ERA5_input.nc\").squeeze()[[\"u_component_of_wind\", \"v_component_of_wind\"]].drop_vars(\"batch\")\n",
    "# dataset3 = dataset3.rename({\"u_component_of_wind\":\"u\", \"v_component_of_wind\":\"v\"})\n",
    "# dataset3.to_netcdf(\"/geodata2/Gencast/era5_2021-06-21.nc\")\n",
    "dataset3 = xr.open_dataset(\"/geodata2/Gencast/era5_divor_2021-06-21.nc\")\n",
    "dataset3\n",
    "#  cdo remapbil,n360 pangu1000_ensavg.nc tmp.nc\n",
    "#  cdo uv2dv,linear tmp.nc pangu1000_dv.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u = dataset['u']\n",
    "print(u.shape)\n",
    "v = dataset['v']\n",
    "\n",
    "dataset_ensavg = dataset.mean(dim=\"ens\")\n",
    "u_mean = dataset_ensavg['u']\n",
    "print(u_mean.shape)\n",
    "v_mean = dataset_ensavg['v']\n",
    "u.time[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u_hat = np.fft.rfft(u, axis=u.dims.index('lon'))\n",
    "v_hat = np.fft.rfft(v, axis=v.dims.index('lon'))\n",
    "\n",
    "u_mean_hat = np.fft.rfft(u_mean, axis=u_mean.dims.index('lon'))\n",
    "v_mean_hat = np.fft.rfft(v_mean, axis=v_mean.dims.index('lon'))\n",
    "\n",
    "# shape of x_hat = (5, 73, 721, 1440); (ens, time, lat, lon)\n",
    "# shape of x_mean_hat = (73, 721, 1440); (time, lat, lon)\n",
    "print(u_hat.shape)\n",
    "print(u_mean_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_i = \\frac{1}{2}\\left(\\lvert\\hat{u}\\rvert^2 + \\lvert\\hat{v}\\rvert^2\\right)$$\n",
    "$$e_{mean} = \\frac{1}{N}\\sum_i e_i(k, \\phi)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_i = 0.5*(np.abs(u_hat)**2 + np.abs(v_hat)**2) # (5, 73, 721, 1440) = (ens, time, lat, lon)\n",
    "e_mean = np.mean(e_i, axis=3) # (73, 721, 1440) = (time, lat, lon)\n",
    "e_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\overline{\\tilde{e}}=\\frac{1}{2}\\left(\\lvert\\langle\\tilde{u}_i(k, \\phi)\\rangle\\rvert^2+\\lvert\\langle\\tilde{v}_i(k, \\phi)\\rangle\\rvert^2\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_mean_field = 0.5*(np.abs(u_mean_hat)**2 + np.abs(v_mean_hat)**2) # (73, 721, 1440)\n",
    "e_mean_field.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Delta \\tilde{e}(k, \\phi) = \\frac{N}{N-1}(\\langle e_i(k, \\phi)\\rangle - \\overline{e}(k, \\phi))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = u.shape[3] # 5\n",
    "print(N)\n",
    "DKE = (N/(N-1))*(e_mean - e_mean_field) # (73, 721, 1440)\n",
    "DKE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dim = DKE.shape[2]\n",
    "wavenumbers = np.arange(1, k_dim+1)\n",
    "DKE_spectra = DKE * np.expand_dims(wavenumbers, axis=(0,1)) \n",
    "\n",
    "phi = np.deg2rad(dataset['lat'])\n",
    "weights = np.cos(phi)\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "weights_3d = np.expand_dims(weights, axis=(0,-1))\n",
    "\n",
    "DKE_global = (DKE * weights_3d).mean(axis=1)\n",
    "\n",
    "# DKE_k = DKE.mean(axis=1)\n",
    "# nlon = u.sizes['lon']\n",
    "# wavenumbers = np.arange(nlon)\n",
    "# DKE_k = wavenumbers * DKE_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = xr.open_zarr(\"/geodata2/Gencast/output/2021-06-21.zarr\").sel(level=300).squeeze()\n",
    "dataset = xr.open_dataset('/home/hiskim1/graphcast/spectra/dataset_paper/pangu1000/pangu1000.nc').squeeze()\n",
    "\n",
    "ens = \"ens\"\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = dataset['u']\n",
    "v = dataset['v']\n",
    "u = dataset['10m_u_component_of_wind']\n",
    "v = dataset['10m_v_component_of_wind']\n",
    "\n",
    "print(u.shape) # (5, 73, 721, 1440) = (ens, time, lat, lon)\n",
    "\n",
    "n = np.sqrt(u.shape[u.dims.index(\"lon\")])\n",
    "\n",
    "tilde_u = np.fft.fft(u,axis=u.dims.index(\"lon\"))/n\n",
    "tilde_v = np.fft.fft(v,axis=u.dims.index(\"lon\"))/n\n",
    "\n",
    "tilde_u.shape # (5, 73, 721, 721) = (ens, time, lat, wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KE Spectra\n",
    "$$\n",
    "\\tilde{e}(k)= \\frac{k}{2}(\\langle \\lvert \\tilde u_i\\rvert^2\\rangle + \\langle \\lvert \\tilde v_i\\rvert^2\\rangle)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KE_spectra = 0.5 * (tilde_u * np.conj(tilde_u)).mean(axis=u.dims.index(ens)) + 0.5 * (tilde_v * np.conj(tilde_v)).mean(axis=u.dims.index(ens))\n",
    "\n",
    "KE_spectra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DKE Spectra\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta\\tilde e  &= k(\\text{var}(\\tilde u) + \\text{var}(\\tilde v) )\\\\\n",
    "&= k\\frac{N}{N-1}(\\underbrace{\\langle \\lvert \\tilde u_i\\rvert^2\\rangle + \\langle \\lvert \\tilde v_i\\rvert^2\\rangle}_{\\text{ensemble mean of the KE spectral densities}} - \\underbrace{(\\langle \\lvert \\tilde u_i\\rvert\\rangle^2 + \\langle \\lvert \\tilde v_i\\rvert\\rangle^2)}_{\\text{KE spectral density of the ensemble mean}})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_tilde_u = tilde_u.var(axis=u.dims.index(ens), ddof=1)\n",
    "var_tilde_v = tilde_v.var(axis=v.dims.index(ens), ddof=1)\n",
    "\n",
    "DKE_spectra_1 = var_tilde_u + var_tilde_v\n",
    "\n",
    "var_tilde_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_2_mean = ((tilde_u * np.conj(tilde_u))).mean(axis=u.dims.index(ens))\n",
    "v_2_mean = ((tilde_v * np.conj(tilde_v))).mean(axis=v.dims.index(ens))\n",
    "\n",
    "u_mean_2 = tilde_u.mean(axis=u.dims.index(ens)) * np.conj(tilde_u.mean(axis=u.dims.index(ens)))\n",
    "v_mean_2 = tilde_v.mean(axis=v.dims.index(ens)) * np.conj(tilde_v.mean(axis=u.dims.index(ens)))\n",
    "\n",
    "N = u.shape[u.dims.index(ens)]\n",
    "bias = N/(N-1)\n",
    "\n",
    "DKE_spectra_2 = bias * (u_2_mean + v_2_mean - u_mean_2 - v_mean_2)\n",
    "DKE_spectra_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight by cos(lat)\n",
    "\n",
    "spectra = DKE_spectra_2\n",
    "print(spectra.shape)\n",
    "phi = np.deg2rad(dataset['lat'])\n",
    "weight = np.cos(phi)\n",
    "\n",
    "k = ((np.arange(spectra.shape[2])+1) * 2 * np.pi * 6317)/spectra.shape[2]\n",
    "map = np.outer(weight, k)\n",
    "\n",
    "# print(map.shape)\n",
    "# spectra = spectra * map\n",
    "# k = np.arange(spectra.shape[2]) + 1\n",
    "# print(k)\n",
    "# spectra = spectra * np.flip(k)\n",
    "\n",
    "weights = weight / weight.sum()\n",
    "\n",
    "weights_3d = np.expand_dims(weights, axis=(0 ,-1))\n",
    "print(weights_3d.shape)\n",
    "\n",
    "spectra = spectra * weights_3d\n",
    "print(spectra.shape)\n",
    "\n",
    "DKE_global = spectra.mean(axis=1)\n",
    "\n",
    "DKE_global.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "colors = plt.cm.cividis(np.linspace(0, 1, 60))\n",
    "\n",
    "data = DKE_global[0]*0.5\n",
    "\n",
    "# Initialize figure with research-quality specifications\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "# Calculate wavelength array (assuming spherical harmonics)\n",
    "n360_circumference = 40075  # Earth's circumference in km\n",
    "wavenumbers = np.arange(1, len(DKE_global[0])+1)\n",
    "wavelengths = n360_circumference / wavenumbers  # Convert to wavelengths in km\n",
    "\n",
    "# Create loglog plot with reversed x-axis\n",
    "# plt.loglog(10**log_k, DKE_log_defined, 'k-', linewidth=1.5, label='Total KE')\n",
    "\n",
    "for i in range(0, 60, 4):\n",
    "    data = DKE_global[i]*0.5\n",
    "    plt.loglog(wavelengths, \n",
    "               data, \n",
    "               color=colors[i], \n",
    "               # label=f'{(i+1)*12}h',\n",
    "               label=f\"{i}h\"\n",
    "               )\n",
    "\n",
    "# Configure axes\n",
    "# plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# Set labels with units\n",
    "plt.xlabel('wavelength [km]')\n",
    "plt.ylabel('spectral density [m²s⁻²]')\n",
    "# plt.title('Pangu-100%')\n",
    "\n",
    "# Set axis limits to match reference plot\n",
    "# plt.xlim(70, 2.5*10**4)\n",
    "plt.gca().invert_xaxis()  # Reverse x-axis direction\n",
    "# plt.ylim(10**-3, 2*10**2)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"dke_iconlr1000.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# From Vorticity and Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NetCDF: HDF error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/geodata2/S2S/DL/GC_input/2021-06-21/ERA5_input.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# change the order of the dimensions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# data = data.transpose(\"time\", \"level\", \"lat\", \"lon\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_component_of_wind\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_component_of_wind\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_netcdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraphcast_input.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/dataset.py:2303\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   2300\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_netcdf(  \u001b[38;5;66;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;00m\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2305\u001b[0m     path,\n\u001b[1;32m   2306\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   2307\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m   2308\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m   2309\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m   2310\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   2311\u001b[0m     unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims,\n\u001b[1;32m   2312\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[1;32m   2313\u001b[0m     multifile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2314\u001b[0m     invalid_netcdf\u001b[38;5;241m=\u001b[39minvalid_netcdf,\n\u001b[1;32m   2315\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py:1315\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1315\u001b[0m     dump_to_store(\n\u001b[1;32m   1316\u001b[0m         dataset, store, writer, encoding\u001b[38;5;241m=\u001b[39mencoding, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims\n\u001b[1;32m   1317\u001b[0m     )\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[1;32m   1319\u001b[0m         store\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py:1362\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[1;32m   1360\u001b[0m     variables, attrs \u001b[38;5;241m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1362\u001b[0m store\u001b[38;5;241m.\u001b[39mstore(variables, attrs, check_encoding, writer, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/common.py:356\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_variables(\n\u001b[1;32m    357\u001b[0m     variables, check_encoding_set, writer, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims\n\u001b[1;32m    358\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/common.py:398\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.set_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    393\u001b[0m check \u001b[38;5;241m=\u001b[39m vn \u001b[38;5;129;01min\u001b[39;00m check_encoding_set\n\u001b[1;32m    394\u001b[0m target, source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_variable(\n\u001b[1;32m    395\u001b[0m     name, v, check, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims\n\u001b[1;32m    396\u001b[0m )\n\u001b[0;32m--> 398\u001b[0m writer\u001b[38;5;241m.\u001b[39madd(source, target)\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/common.py:243\u001b[0m, in \u001b[0;36mArrayWriter.add\u001b[0;34m(self, source, target, region)\u001b[0m\n\u001b[1;32m    241\u001b[0m     target[region] \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     target[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m source\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:78\u001b[0m, in \u001b[0;36mBaseNetCDF4Array.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m     77\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 78\u001b[0m     data[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mautoclose:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mclose(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5519\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable.__setitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5802\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable._put\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2034\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NetCDF: HDF error"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiskim1_gencast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
