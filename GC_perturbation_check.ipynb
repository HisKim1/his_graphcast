{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Header\n",
    "\n",
    "No need to change I guess...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33723 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x14b4b8ee5690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "\n",
    "PRESSURE_VARIABLES=[\n",
    "    \"geopotential\",\n",
    "    \"u_component_of_wind\",\n",
    "    \"v_component_of_wind\",\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    "    \"vertical_velocity\"\n",
    "]\n",
    "\n",
    "SURFACE_VARIABLES=[\n",
    "    \"10m_u_component_of_wind\", \n",
    "    \"10m_v_component_of_wind\", \n",
    "    \"total_precipitation_6hr\", \n",
    "    \"mean_sea_level_pressure\"\n",
    "]\n",
    "\n",
    "# Define the weighted_mean function (ensure it's properly defined in your code)\n",
    "def weighted_mean(dataset: xr.Dataset):\n",
    "    # for GC_* files\n",
    "    if \"level\" in dataset.dims:\n",
    "        dataset = dataset.resample(time=\"1D\").mean().squeeze('batch')\n",
    "        dataset = dataset.drop_vars(PRESSURE_VARIABLES + SURFACE_VARIABLES + ['level'])\n",
    "        dataset[\"time\"] = pd.date_range(\"2021-06-22\", periods=7, freq=\"1D\")\n",
    "        dataset = dataset.rename({\"time\":\"date\"})\n",
    "        # dataset = dataset.sel(lat=slice(25, 60), lon=slice(102.5, 150))\n",
    "    \n",
    "    # for nwp files\n",
    "    elif \"time\" in dataset.dims:\n",
    "        dataset = dataset.expand_dims(dim={'date': [dataset.time.values[0]]}).compute()\n",
    "        dataset = dataset.rename({'time': 'ensemble'})\n",
    "        dataset['ensemble'] = np.arange(1, 51)\n",
    "        dataset = dataset.drop_vars('height')\n",
    "        # dataset = dataset.sel(lat=slice(60, 24), lon=slice(102, 150))\n",
    "    \n",
    "    # for era5 files\n",
    "    # else:\n",
    "        # dataset = dataset.sel(lat=slice(60, 25), lon=slice(102.5, 150))\n",
    "    \n",
    "    weights = np.cos(np.deg2rad(dataset.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weighted = dataset.weighted(weights)\n",
    "    \n",
    "    return weighted.mean(('lat', 'lon'))\n",
    "\n",
    "client = Client()\n",
    "dask.config.set({\"array.query-planning\": False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition samples in reasonable way\n",
    "\n",
    "1. **t2m**\n",
    "    1. all + all + global  $= 4 \\times 23 \\times 1 = 92$  \n",
    "    2. all + only t2m + all $= 4 \\times 1 \\times 12 = 48$  \n",
    "    3. all + except t2m + all $= 4 \\times 1 \\times 12 = 48$  \n",
    "2. **500hPa geopotential height**\n",
    "    1. all + all + global  $= 4 \\times 23 \\times 1 = 92$  \n",
    "    2. all + only geopotential + all $= 4 \\times 1 \\times 12 = 48$  \n",
    "    3. all + except geopotential + all $= 4 \\times 1 \\times 12 = 48$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = '2m_temperature'\n",
    "\n",
    "if target_var == '2m_temperature':\n",
    "    p_1 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_???????????_global_scale*.nc'))\n",
    "    p_2 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_00100000000_*_scale*.nc'))\n",
    "    p_3 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_11011111111_*_scale*.nc'))\n",
    "    \n",
    "elif target_var == 'geopotential':\n",
    "    p_1 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_???????????_global_scale*.nc'))\n",
    "    p_2 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_00000100000_*_scale*.nc'))\n",
    "    p_3 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_11111011111_*_scale*.nc'))    \n",
    "\n",
    "p_4 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_11111111111_global_scale_*.nc'))\n",
    "len(p_2)\n",
    "\n",
    "files_t2m = sorted(glob.glob('/geodata2/S2S/ECMWF_Perturbed/Dailyaveraged/t2m/nc/*/Temperature2m_2021-06-21.nc'))\n",
    "files_t2m = [f for f in files_t2m if int(f.split('/')[-2]) <= 24 * 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading test code \n",
    "\n",
    "정말 빨라지는지 확인해보자ㅏㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 12:32:08,214 - distributed.worker - ERROR - Compute Failed\n",
      "Key:       process_file-fea8f669-e941-4086-978a-5f13aeb5fd22\n",
      "State:     executing\n",
      "Function:  process_file\n",
      "args:      ('/data/GC_output/2021-06-21/GC_11111111111_global_scale_1.nc')\n",
      "kwargs:    {}\n",
      "Exception: 'TypeError(\"manager must be a string or instance of ChunkManagerEntrypoint, but received type <class \\'xarray.core.daskmanager.DaskManager\\'>\")'\n",
      "Traceback: '  File \"/tmp/ipykernel_1635224/2442268749.py\", line 37, in process_file\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py\", line 677, in open_dataset\\n    ds = _dataset_from_backend_dataset(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py\", line 401, in _dataset_from_backend_dataset\\n    ds = _chunk_ds(\\n         ^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py\", line 366, in _chunk_ds\\n    variables[name] = _maybe_chunk(\\n                      ^^^^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/dataset.py\", line 311, in _maybe_chunk\\n    chunked_array_type = guess_chunkmanager(\\n                         ^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/namedarray/parallelcompat.py\", line 119, in guess_chunkmanager\\n    raise TypeError(\\n'\n",
      "\n",
      "2024-11-12 12:32:08,215 - distributed.worker - ERROR - Compute Failed\n",
      "Key:       process_file-21d111ba-b328-42eb-9dfe-00810b149907\n",
      "State:     executing\n",
      "Function:  process_file\n",
      "args:      ('/data/GC_output/2021-06-21/GC_11111111111_global_scale_0.001.nc')\n",
      "kwargs:    {}\n",
      "Exception: 'TypeError(\"manager must be a string or instance of ChunkManagerEntrypoint, but received type <class \\'xarray.core.daskmanager.DaskManager\\'>\")'\n",
      "Traceback: '  File \"/tmp/ipykernel_1635224/2442268749.py\", line 37, in process_file\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py\", line 677, in open_dataset\\n    ds = _dataset_from_backend_dataset(\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py\", line 401, in _dataset_from_backend_dataset\\n    ds = _chunk_ds(\\n         ^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py\", line 366, in _chunk_ds\\n    variables[name] = _maybe_chunk(\\n                      ^^^^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/dataset.py\", line 311, in _maybe_chunk\\n    chunked_array_type = guess_chunkmanager(\\n                         ^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hiskim1/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/namedarray/parallelcompat.py\", line 119, in guess_chunkmanager\\n    raise TypeError(\\n'\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "manager must be a string or instance of ChunkManagerEntrypoint, but received type <class 'xarray.core.daskmanager.DaskManager'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     delayed_tasks\u001b[38;5;241m.\u001b[39mappend((label, color, delayed_task))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute datasets in parallel\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m results \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;241m*\u001b[39m[task \u001b[38;5;28;01mfor\u001b[39;00m _, _, task \u001b[38;5;129;01min\u001b[39;00m delayed_tasks])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Reconstruct perturb_datasets with computed datasets\u001b[39;00m\n\u001b[1;32m     51\u001b[0m perturb_datasets \u001b[38;5;241m=\u001b[39m [(label, color, result) \u001b[38;5;28;01mfor\u001b[39;00m (label, color, _), result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(delayed_tasks, results)]\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/dask/base.py:664\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 664\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(file):\n\u001b[0;32m---> 37\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(file, chunks\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m})\n\u001b[1;32m     38\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m weighted_mean(dataset)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py:677\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    670\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    671\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    672\u001b[0m     filename_or_obj,\n\u001b[1;32m    673\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    676\u001b[0m )\n\u001b[0;32m--> 677\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    678\u001b[0m     backend_ds,\n\u001b[1;32m    679\u001b[0m     filename_or_obj,\n\u001b[1;32m    680\u001b[0m     engine,\n\u001b[1;32m    681\u001b[0m     chunks,\n\u001b[1;32m    682\u001b[0m     cache,\n\u001b[1;32m    683\u001b[0m     overwrite_encoded_chunks,\n\u001b[1;32m    684\u001b[0m     inline_array,\n\u001b[1;32m    685\u001b[0m     chunked_array_type,\n\u001b[1;32m    686\u001b[0m     from_array_kwargs,\n\u001b[1;32m    687\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    690\u001b[0m )\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py:401\u001b[0m, in \u001b[0;36m_dataset_from_backend_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    399\u001b[0m     ds \u001b[38;5;241m=\u001b[39m backend_ds\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ds \u001b[38;5;241m=\u001b[39m _chunk_ds(\n\u001b[1;32m    402\u001b[0m         backend_ds,\n\u001b[1;32m    403\u001b[0m         filename_or_obj,\n\u001b[1;32m    404\u001b[0m         engine,\n\u001b[1;32m    405\u001b[0m         chunks,\n\u001b[1;32m    406\u001b[0m         overwrite_encoded_chunks,\n\u001b[1;32m    407\u001b[0m         inline_array,\n\u001b[1;32m    408\u001b[0m         chunked_array_type,\n\u001b[1;32m    409\u001b[0m         from_array_kwargs,\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_tokens,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    413\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(backend_ds\u001b[38;5;241m.\u001b[39m_close)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Ensure source filename always stored in dataset object\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/backends/api.py:366\u001b[0m, in \u001b[0;36m_chunk_ds\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, var \u001b[38;5;129;01min\u001b[39;00m backend_ds\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    365\u001b[0m     var_chunks \u001b[38;5;241m=\u001b[39m _get_chunk(var, chunks, chunkmanager)\n\u001b[0;32m--> 366\u001b[0m     variables[name] \u001b[38;5;241m=\u001b[39m _maybe_chunk(\n\u001b[1;32m    367\u001b[0m         name,\n\u001b[1;32m    368\u001b[0m         var,\n\u001b[1;32m    369\u001b[0m         var_chunks,\n\u001b[1;32m    370\u001b[0m         overwrite_encoded_chunks\u001b[38;5;241m=\u001b[39moverwrite_encoded_chunks,\n\u001b[1;32m    371\u001b[0m         name_prefix\u001b[38;5;241m=\u001b[39mname_prefix,\n\u001b[1;32m    372\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    373\u001b[0m         inline_array\u001b[38;5;241m=\u001b[39minline_array,\n\u001b[1;32m    374\u001b[0m         chunked_array_type\u001b[38;5;241m=\u001b[39mchunkmanager,\n\u001b[1;32m    375\u001b[0m         from_array_kwargs\u001b[38;5;241m=\u001b[39mfrom_array_kwargs\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend_ds\u001b[38;5;241m.\u001b[39m_replace(variables)\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/core/dataset.py:311\u001b[0m, in \u001b[0;36m_maybe_chunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m {dim: chunks[dim] \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m var\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m chunks}\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 311\u001b[0m     chunked_array_type \u001b[38;5;241m=\u001b[39m guess_chunkmanager(\n\u001b[1;32m    312\u001b[0m         chunked_array_type\n\u001b[1;32m    313\u001b[0m     )  \u001b[38;5;66;03m# coerce string to ChunkManagerEntrypoint type\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunked_array_type, DaskManager):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenize\n",
      "File \u001b[0;32m~/.conda/envs/hiskim1_graphcast/lib/python3.11/site-packages/xarray/namedarray/parallelcompat.py:119\u001b[0m, in \u001b[0;36mguess_chunkmanager\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m manager\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanager must be a string or instance of ChunkManagerEntrypoint, but received type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(manager)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: manager must be a string or instance of ChunkManagerEntrypoint, but received type <class 'xarray.core.daskmanager.DaskManager'>"
     ]
    }
   ],
   "source": [
    "# Assign base colors for each partition\n",
    "partition_colors = {\n",
    "    # 'p_1': 'blue',\n",
    "    # 'p_2': 'green',\n",
    "    # 'p_3': 'red',\n",
    "    'p_4': 'purple'\n",
    "}\n",
    "\n",
    "# Function to extract perturbation type and value from filename\n",
    "def extract_perturbation_info(filename):\n",
    "    match = re.search(r'_([01][01][01][01][01][01][01][01][01][01][01])_(.*?)_(scale|wipeout)_([\\d.eE+-]+)\\.nc$', filename)\n",
    "    if match:\n",
    "        var=match.group(1)\n",
    "        region=match.group(2)\n",
    "        perturb_type = match.group(3)\n",
    "        value = match.group(4)\n",
    "        return f\"{var}_{region}_{perturb_type}_{value}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Collect perturbation files with labels and colors\n",
    "perturb_files = []\n",
    "# for partition_name, partition_files in zip(['p_1', 'p_2', 'p_3'], [p_1, p_2, p_3]):\n",
    "for partition_name, partition_files in zip(['p_4'], [p_4]):\n",
    "    base_color = partition_colors[partition_name]\n",
    "    num_files = len(partition_files)\n",
    "    # Generate different shades of the base color\n",
    "    colors = sns.light_palette(base_color, n_colors=num_files + 2)[1:-1]\n",
    "    for i, file in enumerate(partition_files):\n",
    "        perturb_info = extract_perturbation_info(file)\n",
    "        if perturb_info:\n",
    "            label = f\"{partition_name} {perturb_info}\"\n",
    "            color = colors[i % len(colors)]\n",
    "            perturb_files.append((label, color, file))\n",
    "\n",
    "def process_file(file):\n",
    "    dataset = xr.open_dataset(file, chunks={'latitude': 1000, 'longitude': 1000})\n",
    "    dataset = weighted_mean(dataset)\n",
    "    return dataset\n",
    "\n",
    "# Prepare delayed tasks\n",
    "delayed_tasks = []\n",
    "for label, color, file in perturb_files:\n",
    "    delayed_task = delayed(process_file)(file)\n",
    "    delayed_tasks.append((label, color, delayed_task))\n",
    "\n",
    "# Compute datasets in parallel\n",
    "results = compute(*[task for _, _, task in delayed_tasks])\n",
    "\n",
    "# Reconstruct perturb_datasets with computed datasets\n",
    "perturb_datasets = [(label, color, result) for (label, color, _), result in zip(delayed_tasks, results)]\n",
    "\n",
    "# Process nwp data\n",
    "nwp = xr.open_mfdataset(\n",
    "    files_t2m,\n",
    "    combine='by_coords',\n",
    "    chunks={'latitude': 1440, 'longitude': 720},\n",
    "    preprocess=weighted_mean\n",
    ")\n",
    "nwp = nwp.rename({\"2t\": \"2m_temperature\"}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign base colors for each partition\n",
    "partition_colors = {\n",
    "    # 'p_1': 'blue',\n",
    "    # 'p_2': 'green',\n",
    "    # 'p_3': 'red',\n",
    "    'p_4': 'purple'\n",
    "}\n",
    "\n",
    "# Function to extract perturbation type and value from filename\n",
    "def extract_perturbation_info(filename):\n",
    "    match = re.search(r'_([01][01][01][01][01][01][01][01][01][01][01])_(.*?)_(scale|wipeout)_([\\d.eE+-]+)\\.nc$', filename)\n",
    "    if match:\n",
    "        var=match.group(1)\n",
    "        region=match.group(2)\n",
    "        perturb_type = match.group(3)\n",
    "        value = match.group(4)\n",
    "        return f\"{var}_{region}_{perturb_type}_{value}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Collect perturbation files with labels and colors\n",
    "perturb_files = []\n",
    "# for partition_name, partition_files in zip(['p_1', 'p_2', 'p_3'], [p_1, p_2, p_3]):\n",
    "for partition_name, partition_files in zip(['p_4'], [p_4]):\n",
    "    base_color = partition_colors[partition_name]\n",
    "    num_files = len(partition_files)\n",
    "    # Generate different shades of the base color\n",
    "    colors = sns.light_palette(base_color, n_colors=num_files + 2)[1:-1]\n",
    "    for i, file in enumerate(partition_files):\n",
    "        perturb_info = extract_perturbation_info(file)\n",
    "        if perturb_info:\n",
    "            label = f\"{partition_name} {perturb_info}\"\n",
    "            color = colors[i % len(colors)]\n",
    "            perturb_files.append((label, color, file))\n",
    "\n",
    "# Preprocess the perturbation datasets\n",
    "perturb_datasets = []\n",
    "for label, color, file in perturb_files:\n",
    "    dataset = weighted_mean(xr.open_dataset(file))\n",
    "    perturb_datasets.append((label, color, dataset))\n",
    "\n",
    "files_t2m = sorted(glob.glob('/geodata2/S2S/ECMWF_Perturbed/Dailyaveraged/t2m/nc/*/Temperature2m_2021-06-21.nc'))\n",
    "files_t2m = [f for f in files_t2m if int(f.split('/')[-2]) <= 24 * 7]\n",
    "\n",
    "nwp = xr.open_mfdataset(files_t2m, combine='by_coords', preprocess=weighted_mean)\n",
    "nwp = nwp.rename({\"2t\":\"2m_temperature\"}).compute()\n",
    "df = nwp[target_var].to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot ensemble members\n",
    "# first_ensemble = True\n",
    "# for ensemble in df['ensemble'].unique():\n",
    "#     subset = df[df['ensemble'] == ensemble]\n",
    "#     if first_ensemble:\n",
    "#         plt.plot(subset['date'], subset['2m_temperature'], color='grey', linewidth=0.5, alpha=0.5, label='Ensemble Members')\n",
    "#         first_ensemble = False\n",
    "#     else:\n",
    "#         plt.plot(subset['date'], subset['2m_temperature'], color='grey', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# # Plot ensemble mean\n",
    "# mean_temp = df.groupby('date')['2m_temperature'].mean().reset_index()\n",
    "# plt.plot(mean_temp['date'], mean_temp['2m_temperature'], color='black', linewidth=1, label='Ensemble Mean')\n",
    "\n",
    "# # Plot perturbation datasets\n",
    "# for label, color, dataset in perturb_datasets:\n",
    "#     plt.plot(dataset['date'], dataset['2m_temperature'], color=color, linewidth=1, label=label)\n",
    "\n",
    "# # Optional: Plot ERA5 data if available\n",
    "# # era5 = xr.open_dataset(\"/camdata2/ERA5/daily/t2m/2021.nc\").rename({\"time\":\"date\", \"latitude\":\"lat\", \"longitude\":\"lon\"}).sel(date=slice(\"2021-06-22\", \"2021-07-01\"))\n",
    "# # era5 = weighted_mean(era5)\n",
    "# # plt.plot(era5['date'], era5['t2m'], color='red', linewidth=1.5, linestyle='dashed', label='ERA5')\n",
    "\n",
    "# plt.title('Mean 2m Temperature Forecast / 2021-06-21  + 10 days', fontsize=16)\n",
    "# plt.xlabel('Date', fontsize=12)\n",
    "# plt.ylabel('Temperature (K)', fontsize=12)\n",
    "# plt.legend(fontsize=5)\n",
    "\n",
    "# # Adjust y-axis limits based on all datasets\n",
    "# all_temps = []\n",
    "# for _, _, dataset in perturb_datasets:\n",
    "#     all_temps.extend(dataset['2m_temperature'].values)\n",
    "# all_temps.extend(df['2m_temperature'].values)\n",
    "# y_min = min(all_temps) - 2\n",
    "# y_max = max(all_temps) + 2\n",
    "# plt.ylim(y_min, y_max)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# # plt.savefig('figure/2m_temperature_forecast_mean_2021-06-21.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot ensemble members\n",
    "first_ensemble = True\n",
    "for ensemble in df['ensemble'].unique():\n",
    "    subset = df[df['ensemble'] == ensemble]\n",
    "    if first_ensemble:\n",
    "        plt.plot(subset['date'], subset[target_var], color='grey', linewidth=0.5, alpha=0.5, label='Ensemble Members')\n",
    "        first_ensemble = False\n",
    "    else:\n",
    "        plt.plot(subset['date'], subset[target_var], color='grey', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Plot ensemble mean\n",
    "mean_temp = df.groupby('date')[target_var].mean().reset_index()\n",
    "plt.plot(mean_temp['date'], mean_temp[target_var], color='black', linewidth=1, label='Ensemble Mean')\n",
    "\n",
    "# Plot perturbation datasets\n",
    "for label, color, dataset in perturb_datasets:\n",
    "    plt.plot(dataset['date'], dataset[target_var], color=color, linewidth=1, label=label)\n",
    "\n",
    "# Optional: Plot ERA5 data if available\n",
    "# era5 = xr.open_dataset(\"/camdata2/ERA5/daily/t2m/2021.nc\").rename({\"time\":\"date\", \"latitude\":\"lat\", \"longitude\":\"lon\"}).sel(date=slice(\"2021-06-22\", \"2021-07-01\"))\n",
    "# era5 = weighted_mean(era5)\n",
    "# plt.plot(era5['date'], era5['t2m'], color='red', linewidth=1.5, linestyle='dashed', label='ERA5')\n",
    "\n",
    "plt.title('Mean 2m Temperature Forecast / 2021-06-21  + 7 days', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Temperature (K)', fontsize=12)\n",
    "\n",
    "\n",
    "# Adjust y-axis limits based on all datasets\n",
    "all_temps = []\n",
    "for _, _, dataset in perturb_datasets:\n",
    "    all_temps.extend(dataset[target_var].values)\n",
    "all_temps.extend(df[target_var].values)\n",
    "y_min = min(mean_temp[target_var]) - 1\n",
    "y_max = max(mean_temp[target_var]) + 1\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlim([pd.Timestamp('2021-06-22'), pd.Timestamp('2021-06-28')])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "\n",
    "# Plot ensemble members\n",
    "ensemble_lines = []\n",
    "ensemble_legend_shown = False  # Flag to control legend display\n",
    "for ensemble in df['ensemble'].unique():\n",
    "    subset = df[df['ensemble'] == ensemble]\n",
    "    ensemble_lines.append(go.Scatter(\n",
    "        x=subset['date'],\n",
    "        y=subset[target_var],\n",
    "        mode='lines',\n",
    "        line=dict(color='grey', width=0.5),\n",
    "        opacity=0.5,\n",
    "        name='Ensemble Members' if not ensemble_legend_shown else None,\n",
    "        showlegend=not ensemble_legend_shown,\n",
    "        legendgroup='Ensemble Members',\n",
    "        legendgrouptitle_text='Ensemble Members'\n",
    "    ))\n",
    "    ensemble_legend_shown = True  # Only show legend once\n",
    "\n",
    "# Plot ensemble mean\n",
    "mean_temp = df.groupby('date')[target_var].mean().reset_index()\n",
    "ensemble_mean_line = go.Scatter(\n",
    "    x=mean_temp['date'],\n",
    "    y=mean_temp[target_var],\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=1),\n",
    "    name='Ensemble Mean'\n",
    ")\n",
    "\n",
    "# Plot perturbation datasets\n",
    "perturb_lines = []\n",
    "partition_legend_shown = {}  # Dictionary to track legend entries per partition\n",
    "for label, color, dataset in perturb_datasets:\n",
    "    # Extract partition name from label (assuming label starts with partition name)\n",
    "    partition_name = label.split()[0]\n",
    "    # Only show legend once per partition\n",
    "    if partition_name not in partition_legend_shown:\n",
    "        show_legend = True\n",
    "        partition_legend_shown[partition_name] = True\n",
    "    else:\n",
    "        show_legend = False\n",
    "    # Convert color to a valid format if it's a tuple\n",
    "    if isinstance(color, tuple):\n",
    "        color = f'rgb({int(color[0] * 255)}, {int(color[1] * 255)}, {int(color[2] * 255)})'\n",
    "    perturb_lines.append(go.Scatter(\n",
    "        x=dataset['date'],\n",
    "        y=dataset[target_var],\n",
    "        mode='lines',\n",
    "        line=dict(color=color, width=1),\n",
    "        name=partition_name if show_legend else None,\n",
    "        showlegend=show_legend,\n",
    "        legendgroup=partition_name,\n",
    "        legendgrouptitle_text=partition_name\n",
    "    ))\n",
    "\n",
    "# Optional: Plot ERA5 data if available\n",
    "# Uncomment and adjust accordingly if ERA5 data is available\n",
    "# era5_line = go.Scatter(\n",
    "#     x=era5['date'],\n",
    "#     y=era5['t2m'],\n",
    "#     mode='lines',\n",
    "#     line=dict(color='red', width=1.5, dash='dash'),\n",
    "#     name='ERA5',\n",
    "#     legendgroup='ERA5',\n",
    "#     legendgrouptitle_text='ERA5'\n",
    "# )\n",
    "\n",
    "# Combine all traces\n",
    "all_traces = ensemble_lines + [ensemble_mean_line] + perturb_lines  # + [era5_line] if ERA5 data is included\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='Mean 2m Temperature Forecast / 2021-06-21  + 7 days',\n",
    "    xaxis=dict(title='Date', range=[pd.Timestamp('2021-06-22'), pd.Timestamp('2021-06-28')]),\n",
    "    yaxis=dict(title='Temperature (K)'),\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    height=800,  # Increased height for better visibility\n",
    "    width=1200,  # Increased width for better visibility\n",
    "    template='plotly_white',\n",
    "    legend=dict(\n",
    "        title='Legend',\n",
    "        orientation='v',  # Vertical legend\n",
    "        x=1.05,  # Position it just outside the right edge of the plot\n",
    "        y=1,    # Align at the top\n",
    "        itemsizing='constant',  # Makes the legend box size consistent\n",
    "        traceorder='grouped',  # Groups traces in the legend\n",
    "        itemclick='toggle',  # Enables toggling traces on and off\n",
    "        itemdoubleclick='toggleothers'  # Double-clicking will turn other traces off\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=all_traces, layout=layout)\n",
    "\n",
    "# Save the figure as an interactive HTML file\n",
    "fig.write_html(\"interactive_temperature_forecast.html\")\n",
    "\n",
    "# Optional: Show the figure in the browser (still interactive)\n",
    "pio.show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import concurrent.futures\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PRESSURE_VARIABLES = [\n",
    "    \"geopotential\",\n",
    "    \"u_component_of_wind\",\n",
    "    \"v_component_of_wind\",\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    "    \"vertical_velocity\"\n",
    "]\n",
    "\n",
    "SURFACE_VARIABLES = [\n",
    "    \"10m_u_component_of_wind\", \n",
    "    \"10m_v_component_of_wind\", \n",
    "    \"total_precipitation_6hr\", \n",
    "    \"mean_sea_level_pressure\"\n",
    "]\n",
    "\n",
    "def weighted_mean(dataset: xr.Dataset):\n",
    "    if \"level\" in dataset.dims:\n",
    "        dataset = dataset.resample(time=\"1D\").mean().squeeze('batch')\n",
    "        dataset = dataset.drop_vars(PRESSURE_VARIABLES + SURFACE_VARIABLES + ['level'])\n",
    "        dataset[\"time\"] = pd.date_range(\"2021-06-22\", periods=7, freq=\"1D\")\n",
    "        dataset = dataset.rename({\"time\":\"date\"})\n",
    "    \n",
    "    elif \"time\" in dataset.dims:\n",
    "        dataset = dataset.expand_dims(dim={'date': [dataset.time.values[0]]}).compute()\n",
    "        dataset = dataset.rename({'time': 'ensemble'})\n",
    "        dataset['ensemble'] = np.arange(1, 51)\n",
    "        dataset = dataset.drop_vars('height')\n",
    "    \n",
    "    weights = np.cos(np.deg2rad(dataset.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weighted = dataset.weighted(weights)\n",
    "    \n",
    "    return weighted.mean(('lat', 'lon'))\n",
    "\n",
    "def process_single_file(file_info):\n",
    "    \"\"\"단일 파일을 처리하는 함수\"\"\"\n",
    "    label, color, file = file_info\n",
    "    try:\n",
    "        # lazy loading을 위해 chunks parameter 사용\n",
    "        with xr.open_dataset(file, chunks={'time': 1, 'lat': 100, 'lon': 100}) as ds:\n",
    "            result = weighted_mean(ds)\n",
    "        return (label, color, result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Dask 클러스터 설정\n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='8GB')\n",
    "    client = Client(cluster)\n",
    "    print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "\n",
    "    target_var = '2m_temperature'\n",
    "    \n",
    "    # 파일 패턴 정의\n",
    "    if target_var == '2m_temperature':\n",
    "        p_1 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_???????????_global_scale*.nc'))\n",
    "        p_2 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_00100000000_*_scale*.nc'))\n",
    "        p_3 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_11011111111_*_scale*.nc'))\n",
    "    elif target_var == 'geopotential':\n",
    "        p_1 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_???????????_global_scale*.nc'))\n",
    "        p_2 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_00000100000_*_scale*.nc'))\n",
    "        p_3 = sorted(glob.glob('/data/GC_output/2021-06-21/GC_11111011111_*_scale*.nc'))\n",
    "\n",
    "    # 파티션 색상 정의\n",
    "    partition_colors = {\n",
    "        'p_1': 'blue',\n",
    "        'p_2': 'green',\n",
    "        'p_3': 'red',\n",
    "    }\n",
    "\n",
    "    def extract_perturbation_info(filename):\n",
    "        match = re.search(r'_([01][01][01][01][01][01][01][01][01][01][01])_(.*?)_(scale|wipeout)_([\\d.eE+-]+)\\.nc$', filename)\n",
    "        if match:\n",
    "            var = match.group(1)\n",
    "            region = match.group(2)\n",
    "            perturb_type = match.group(3)\n",
    "            value = match.group(4)\n",
    "            return f\"{var}_{region}_{perturb_type}_{value}\"\n",
    "        return None\n",
    "\n",
    "    # 파일 정보 수집\n",
    "    perturb_files = []\n",
    "    for partition_name, partition_files in zip(['p_1', 'p_2', 'p_3'], [p_1, p_2, p_3]):\n",
    "        base_color = partition_colors[partition_name]\n",
    "        num_files = len(partition_files)\n",
    "        colors = sns.light_palette(base_color, n_colors=num_files + 2)[1:-1]\n",
    "        \n",
    "        for i, file in enumerate(partition_files):\n",
    "            perturb_info = extract_perturbation_info(file)\n",
    "            if perturb_info:\n",
    "                label = f\"{partition_name} {perturb_info}\"\n",
    "                color = colors[i % len(colors)]\n",
    "                perturb_files.append((label, color, file))\n",
    "\n",
    "    # 병렬 처리 실행\n",
    "    perturb_datasets = []\n",
    "    chunk_size = 10  # 한 번에 처리할 파일 수\n",
    "    \n",
    "    for i in range(0, len(perturb_files), chunk_size):\n",
    "        chunk = perturb_files[i:i + chunk_size]\n",
    "        \n",
    "        # 청크 단위로 병렬 처리\n",
    "        futures = [dask.delayed(process_single_file)(file_info) for file_info in chunk]\n",
    "        results = dask.compute(*futures)\n",
    "        \n",
    "        # 결과 수집\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                perturb_datasets.append(result)\n",
    "        \n",
    "        print(f\"Processed files {i+1} to {min(i+chunk_size, len(perturb_files))} of {len(perturb_files)}\")\n",
    "\n",
    "    # NWP 파일 처리\n",
    "    files_t2m = sorted(glob.glob('/geodata2/S2S/ECMWF_Perturbed/Dailyaveraged/t2m/nc/*/Temperature2m_2021-06-21.nc'))\n",
    "    files_t2m = [f for f in files_t2m if int(f.split('/')[-2]) <= 24 * 10]\n",
    "    \n",
    "    # NWP 파일도 청크로 나눠서 처리\n",
    "    chunks = {'time': 1, 'lat': 100, 'lon': 100}\n",
    "    nwp = xr.open_mfdataset(files_t2m, \n",
    "                           combine='by_coords', \n",
    "                           preprocess=weighted_mean,\n",
    "                           parallel=True,\n",
    "                           chunks=chunks)\n",
    "    \n",
    "    nwp = nwp.rename({\"2t\": \"2m_temperature\"})\n",
    "    df = nwp[target_var].to_dataframe().reset_index()\n",
    "\n",
    "    # 클러스터 종료\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    return perturb_datasets, df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perturb_datasets, df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiskim1_graphcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
