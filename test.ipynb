{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from google.cloud import storage\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import checkpoint\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import normalization\n",
    "from graphcast import rollout\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import xarray\n",
    "import his_utils\n",
    "\n",
    "def parse_file_parts(file_name):\n",
    "  return dict(part.split(\"-\", 1) for part in file_name.split(\"_\"))\n",
    "\n",
    "gcs_client = storage.Client.create_anonymous_client()\n",
    "gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")\n",
    "gcs_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization \n",
    "# TODO: 여기 값 바꾸면 됨 근데 모델 불러서 쓸거면 상관 없ㅅ음\n",
    "resolution = 0 # 0.25 or 1.0\n",
    "mesh_size = 1000 # 4~6\n",
    "latent_size = 100 # 2^4 ~ 2^9\n",
    "gnn_msg_steps = 10 # 1~32\n",
    "pressure_levels = 37 # 13, 25, 37\n",
    "hidden_layers = 1 \n",
    "radius_query_fraction_edge_length = 0.6 # 1로도 가능\n",
    "params = None\n",
    "state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model_config = graphcast.ModelConfig(\n",
    "        resolution=resolution, \n",
    "        mesh_size=mesh_size,\n",
    "        latent_size=latent_size,\n",
    "        gnn_msg_steps=gnn_msg_steps,\n",
    "        hidden_layers=hidden_layers,\n",
    "        radius_query_fraction_edge_length=radius_query_fraction_edge_length    \n",
    "    )\n",
    "\n",
    "task_config = graphcast.TaskConfig(\n",
    "        input_variables=(graphcast.TARGET_SURFACE_VARS + graphcast.TARGET_ATMOSPHERIC_VARS + graphcast.FORCING_VARS +\n",
    "        graphcast.STATIC_VARS),\n",
    "        target_variables=graphcast.TARGET_SURFACE_VARS + graphcast.TARGET_ATMOSPHERIC_VARS,\n",
    "        forcing_variables=graphcast.FORCING_VARS,\n",
    "        pressure_levels=graphcast.PRESSURE_LEVELS[pressure_levels],\n",
    "        input_duration=\"12h\"\n",
    "    )\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisits\n",
    "\n",
    "GC_original = 'GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz'\n",
    "GC_operational = 'GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz'\n",
    "GC_small = 'GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz'\n",
    "\n",
    "# TODO: 모델 하나 골라 로드하기\n",
    "pretrained_model = GC_original\n",
    "\n",
    "with gcs_bucket.blob(f\"params/{pretrained_model}\").open(\"rb\") as f:\n",
    "    ckpt = checkpoint.load(f, graphcast.CheckPoint)\n",
    "\n",
    "    \n",
    "params = ckpt.params  # 로드된 체크포인트에서 파라미터 가져오기\n",
    "state = {}  # 초기 상태는 빈 딕셔너리로 설정\n",
    "\n",
    "# 체크포인트에서 모델 구성 가져오기\n",
    "model_config = ckpt.model_config\n",
    "# 체크포인트에서 작업 구성 가져오기\n",
    "task_config = ckpt.task_config\n",
    "\n",
    "# 모델 설명 출력\n",
    "print(\"Model description:\\n\", ckpt.description, \"\\n\")\n",
    "# 모델 라이선스 출력\n",
    "print(\"Model license:\\n\", ckpt.license, \"\\n\")\n",
    "\n",
    "print(\"Model config:\\n\", model_config, \"\\n\")\n",
    "\n",
    "print(\"Task config:\\n\", task_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "scale =5\n",
    "\n",
    "file_name = f'testdata/2021-06-21/ERA5_input.nc'\n",
    "\n",
    "dataset = xarray.open_dataset(file_name)\n",
    "dataset\n",
    "# print(\"TOA_solar_incident_radiation\" in dataset.data_vars)\n",
    "# dataset.drop_vars(\"TOA_solar_incident_radiation\") #       <---- TOA 사용 여부 변경!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점검\n",
    "\n",
    "# 데이터셋의 시간 차원이 최소 3인지 확인 (입력용 2, 목표용 1 이상)\n",
    "# assert dataset.dims[\"time\"] >= 3  # 2 for input, >=1 for targets\n",
    "\n",
    "# 데이터셋의 시간 크기 확인\n",
    "total_time_steps = dataset.sizes[\"time\"] - 2\n",
    "\n",
    "total_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "# train_steps = 0\n",
    "eval_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "#     dataset, \n",
    "#     target_lead_times=slice(\"6h\", f\"{train_steps * 6}h\"), \n",
    "#     **dataclasses.asdict(task_config)\n",
    "# )\n",
    "\n",
    "eval_inputs, _, _ = data_utils.extract_inputs_targets_forcings(\n",
    "    dataset, \n",
    "    target_lead_times=slice(\"6h\", f\"{eval_steps * 6}h\"), \n",
    "    **dataclasses.asdict(task_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행\n",
    "\n",
    "데이터 뽑기는 여기 앞까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the example batch and extract|ed training and evaluation data\n",
    "print(\"All Examples:  \", dataset.dims.mapping)\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Train Inputs:  \", train_inputs.dims.mapping)\n",
    "# print(\"Train Targets: \", train_targets.dims.mapping)\n",
    "# print(\"Train Forcings:\", train_forcings.dims.mapping)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Eval Inputs:   \", eval_inputs.dims.mapping)\n",
    "# print(\"Eval Targets:  \", eval_targets.dims.mapping)\n",
    "# print(\"Eval Forcings: \", eval_forcings.dims.mapping)\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Eval Inputs:   \\n\", eval_inputs)\n",
    "# print(\"Eval Targets:  \\n\", eval_targets)\n",
    "# print(\"Eval Forcings: \\n\", eval_forcings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load normalization data\n",
    "\n",
    "diffs_stddev_by_level = xarray.open_dataset(\"testdata/stats/stats_diffs_stddev_by_level.nc\")\n",
    "\n",
    "mean_by_level = xarray.open_dataset(\"testdata/stats/stats_mean_by_level.nc\")\n",
    "\n",
    "stddev_by_level = xarray.open_dataset(\"testdata/stats/stats_stddev_by_level.nc\")\n",
    "\n",
    "\n",
    "eval_steps = 40\n",
    "\n",
    "target_template = his_utils.create_target_dataset(time_steps=eval_steps, \n",
    "                   resolution=model_config.resolution, \n",
    "                   pressure_levels=len(task_config.pressure_levels))\n",
    "\n",
    "# start_time: dataset.datetime[0]\n",
    "forcings = his_utils.create_forcing_dataset(time_steps=eval_steps,\n",
    "                                              resolution=model_config.resolution,\n",
    "                                              start_time=dataset.datetime.values[0,0])\n",
    "target_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT 컴파일된 함수를 생성하고, 필요한 경우 랜덤 가중치를 초기화합니다\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"GraphCast 예측기를 구성하고 래핑합니다.\"\"\"\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "    \"\"\"구성된 예측기를 사용하여 순전파를 실행합니다.\"\"\"\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "    \"\"\"구성된 예측기를 사용하여 손실과 진단 정보를 계산합니다.\"\"\"\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "    return xarray_tree.map_structure(\n",
    "        lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "        (loss, diagnostics)\n",
    "    )\n",
    "\n",
    "def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):\n",
    "  \"\"\"매개변수에 대한 손실 함수의 그래디언트를 계산합니다.\"\"\"\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "def with_configs(fn):\n",
    "  \"\"\"모델 및 작업 구성을 적용하는 유틸리티 함수입니다.\"\"\"\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "def with_params(fn):\n",
    "  \"\"\"함수가 항상 매개변수와 상태를 받도록 보장합니다.\"\"\"\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "def drop_state(fn):\n",
    "  \"\"\"예측만 반환합니다. 롤아웃 코드에 필요합니다.\"\"\"\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "    # 매개변수와 상태가 아직 설정되지 않은 경우 초기화합니다.\n",
    "    params, state = init_jitted(\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=train_inputs,\n",
    "        targets_template=train_targets,\n",
    "        forcings=train_forcings\n",
    "    )\n",
    "\n",
    "# 필요한 구성과 매개변수로 함수들을 JIT 컴파일합니다.\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))\n",
    "\n",
    "# Ensure that the model resolution matches the data resolution\n",
    "assert model_config.resolution in (0, 360. / eval_inputs.sizes[\"lon\"]), (\n",
    "    \"Model resolution doesn't match the data resolution. You likely want to \"\n",
    "    \"re-filter the dataset list, and download the correct data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.__version__)\n",
    "print(jax.devices())\n",
    "\n",
    "# Perform autoregressive rollout to generate predictions\n",
    "predictions = rollout.chunked_prediction(\n",
    "    run_forward_jitted,\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=eval_inputs,\n",
    "    targets_template=target_template,\n",
    "    forcings=forcings\n",
    ")\n",
    "\n",
    "# Display the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_netcdf(f\"testdata/2021-06-26/prediction_40step_{scale}std_perturb.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot\n",
    "\n",
    "이쁘게 포장하는 공정은 여기부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import his_utils, his_plot\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "target_var_list = [\n",
    " '2m_temperature']\n",
    "\n",
    "dataset = xr.open_dataset(\"testdata/stats/40yr_std_daily.nc\")\n",
    "dataset = his_utils.convert_scale(dataset)\n",
    "dataset[\"2m_temperature\"].dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in target_var_list:\n",
    "    weights = np.cos(np.deg2rad(dataset.lat))\n",
    "    weights.name = \"weights\"\n",
    "    weighted = dataset[var].weighted(weights)\n",
    "    mean_val = dataset[var].mean(('lat', 'lon')).max()\n",
    "    std_val = dataset[var].std(('lat', 'lon')).max()\n",
    "\n",
    "    norm = TwoSlopeNorm(vcenter=mean_val)\n",
    "\n",
    "    title = \"std\"\n",
    "\n",
    "    arg_list = [(dataset[var].isel(hour=time_index), \n",
    "                 var, \n",
    "                 \"platecarree\", \n",
    "                 \"RdBu_r\", \n",
    "                 f'{var} {title} at t:{time_index}', \n",
    "                 f'figure/std {var}_{time_index}.png',\n",
    "                 norm)\n",
    "                   for time_index in range(len(dataset.hour))]\n",
    "        \n",
    "    with Pool() as pool:\n",
    "        pool.map(his_plot.plot, arg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 움짤 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import his_utils\n",
    "\n",
    "# 'figure' 디렉토리의 경로를 지정합니다.\n",
    "figure_dir = 'figure'\n",
    "\n",
    "# # 파일 이름에서 날짜와 시간을 추출하는 정규표현식\n",
    "# pattern = r'polar_GC_temperature_(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}).png'\n",
    "\n",
    "# # 파일 이름에서 날짜와 시간을 추출하여 리스트를 만듭니다.\n",
    "# date_list = sorted([re.search(pattern, f).group(1) for f in os.listdir(figure_dir) if f.startswith('polar_GC_temperature_') and f.endswith('.png')])\n",
    "\n",
    "# 정렬된 date_list를 사용하여 image_frames를 생성합니다.\n",
    "image_frames = [Image.open(os.path.join(figure_dir, f'total_precipitation_6hrRR_{date}.png')) for date in range(0,14)]\n",
    "\n",
    "# GIF를 저장합니다.\n",
    "his_utils.save_gif(image_frames, 'tp_6hr diff.gif', duration=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import his_plot\n",
    "\n",
    "surface_list = [\n",
    "    '10m_u_component_of_wind',\n",
    "    '10m_v_component_of_wind',\n",
    "    '2m_temperature',\n",
    "    'mean_sea_level_pressure',\n",
    "    'total_precipitation_6hr'\n",
    "]\n",
    "\n",
    "pressure_list = [\n",
    "    'geopotential',\n",
    "    'specific_humidity',\n",
    "    'temperature',\n",
    "    'u_component_of_wind',\n",
    "    'v_component_of_wind',\n",
    "    'vertical_velocity',\n",
    "]\n",
    "\n",
    "title = \"ERA5 prediction: 1st - 2nd\"\n",
    "\n",
    "def process_var(args):\n",
    "    var, duration, level = args\n",
    "    image_frames = []\n",
    "    for time_index in range(8):\n",
    "        if level is not None:\n",
    "            filename = f'{title} {var}_{time_index}_{level}.png'\n",
    "        else:\n",
    "            filename = f'{title} {var}_{time_index}.png'\n",
    "        \n",
    "        file_path = Path(\"figure\") / filename\n",
    "        if file_path.exists():\n",
    "            with Image.open(file_path) as img:\n",
    "                image_frames.append(img.copy())\n",
    "        else:\n",
    "            print(f\"Warning: File not found - {file_path}\")\n",
    "    \n",
    "    if image_frames:\n",
    "        output_filename = f'2022-01-01_{var}.gif'\n",
    "        if level is not None:\n",
    "            output_filename = f'2022-01-01_{var}_{level}.gif'\n",
    "        his_plot.save_gif(image_frames, output_filename, duration=duration)\n",
    "    else:\n",
    "        print(f\"No images found for variable: {var}\")\n",
    "\n",
    "duration = 700\n",
    "with Pool() as pool:\n",
    "    # Process surface variables\n",
    "    surface_args = [(var, duration, None) for var in surface_list]\n",
    "    pool.map(process_var, surface_args)\n",
    "    \n",
    "    # Process pressure variables\n",
    "    pressure_args = [(var, duration, level) \n",
    "                     for level in range(37) \n",
    "                     for var in pressure_list]\n",
    "    pool.map(process_var, pressure_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "ERA5 = xr.open_dataset(f'testdata/2022-01-01RR.nc')\n",
    "google = xr.open_dataset('testdata/source-era5_date-2022-01-01_res-0.25_levels-37_steps-12.nc').drop_vars('toa_incident_solar_radiation')\n",
    "\n",
    "def plot(args):\n",
    "    dataset, target_var, time_index = args\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
    "\n",
    "    data = dataset * 1000\n",
    "\n",
    "    im = ax.pcolormesh(data.lon, data.lat, data.isel(time=time_index).squeeze(), \n",
    "                   transform=ccrs.PlateCarree(), \n",
    "                   cmap='Blues',\n",
    "                   norm=TwoSlopeNorm(vmin=0, vcenter=0.5, vmax=20),\n",
    "                   shading='auto')\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    flag = 'batch' in dataset.coords\n",
    "\n",
    "    plt.title(f'{flag} {target_var}\\nTime: {time_index}')\n",
    "    ax.set_global()\n",
    "\n",
    "    plt.savefig(f'figure/{flag} {target_var}_{time_index}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "with Pool() as pool:\n",
    "    args_list = [(ERA5['total_precipitation_6hr'], 'total_precipitation_6hr',time_index) for time_index in range(0, 14)]\n",
    "    pool.map(plot, args_list)\n",
    "\n",
    "with Pool() as pool:\n",
    "    args_list = [(google['total_precipitation_6hr'], 'total_precipitation_6hr', time_index) for time_index in range(0, 14)]\n",
    "    pool.map(plot, args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "google_mean = xr.open_dataset(\"testdata/stats_mean_by_level.nc\")\n",
    "google_mean.data_vars\n",
    "VAR_LIST = [\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",    \n",
    "    \"geopotential\",\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    "    \"u_component_of_wind\",\n",
    "    \"v_component_of_wind\",\n",
    "    \"vertical_velocity\",\n",
    "]\n",
    "\n",
    "# for var in VAR_LIST:\n",
    "#     print(var)\n",
    "#     print(google_mean[var].values)\n",
    "\n",
    "google_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiskim1_graphcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
